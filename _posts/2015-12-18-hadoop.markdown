---
layout: post
title:  "Hadoop keypoints for beginners"
date:   2013-11-10 10:18:00
categories: dev,datascience
---

**Hadoop default configuration**

* `/etc/hadoop/conf`

**Jobs management**

1. Master sends the actual `jar` to be executed to data nodes.
1. Hadoop sends data from mappers to reducers even before `mappers` finished in order to have `reducers` be able to already start working.
1. `hadoop jar` is the command that will run your jar (you should have already uploaded it to hadoop cluster with `-put`
1. Show list of running jobs `mapred job -list`

**Hadoop Map Reduce**

1. `FileInputFormat` takes a file and if we have multiple blocks splits it to multiple mappers.
1. You should have a main (not mapper and not reducer which is your `main`) it receives the command line parameters uses the `FileInputFormat` to split the file (which has usually multiple `blocks` to multiple `mappers`.  The `FileInputFormat` uses the `InputPath`. We have similarly `FileOutputFormat`.
1. When referring to `Strings` for example in output you refer to `job.setOutputKeyClass(Text.class)`
1. You can give the `inputPath` multiple `*` for all files in directory or all directories or just use the `Jobs` api to add multiple paths.
1. You have access to main `Job` object which handles the job `from` your `main`.
1. The `id` of the `key` object brought to `mapper` is the offset from file (it has also the actual `key`).
1. In mapper you use `context.write` in order to write the result (`context` is parameter to `mapper`).
1. If you write a string as output from mapper you write it with `new Text(somestr)`
1. `mapper` is a pure function it gets input `key, value` and emits `key, value` or multiple keys and values.  So as its as much pure as possible its not intended to perform states meaning it will not combine results for mapping internally (see `combiners` if you need that).
1. `Reducers` receive a `key -> values` it will get called multiple times with **sorted** `keys`.

**Example Hadoop Map Reduce**

You need to have 3 files.

1. Job manager.
2. Mapper.
3. Reducer.

Let's see each of them in an exmaple.

*1. JobManager*

{% highlight java %}
  /**
   * Setup the job.
   */
  public static void main(String[] args) throws Exception {

    // inputs
    if (args.length != 2) {
      System.out.printf("Usage: YourJob <input dir> <output dir>\n");
      System.exit(-1);
    }

    // Set job inputs outputs.
    Job job = new Job();
    job.setJarByClass(YourJob.class);
    job.setJobName("Your Job Length");
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    job.setMapperClass(YourMapper.class);
    job.setReducerClass(YourReducer.class);
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(DoubleWritable.class);

    // exit..
    boolean success = job.waitForCompletion(true);
    System.exit(success ? 0 : 1);
  }
{% endhighlight %}

*2. YourMapper*

{%highlight java%}
  public class YourMapper extends Mapper<LongWritable, Text, Text, IntWritable> { // <InputKey, InputValue, OutputKey, OutputValue>

    @Override
    public void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {

  	  final String line = value.toString();
  	  for (String word: line.split("\\W+")) {
         context.write(context, new Text(word), new IntWritable(1)); // write mapper output.
  	  }
    }
{%endhighlight%}

*3. YourReducer*

{%highlight java%}
public class YourReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

  /**
   * Note you get a key and then a list of all values which were emitted for this key.
   * The keys which are handed to reducers are sorted.
   */
  @Override
  public void reduce(Text key, Iterable<IntWritable> values, Context context)
      throws IOException, InterruptedException {

      int count = 0;
	  for (IntWritable value: values) {
		  count++;
	  }
      context.write(new Text(key), new IntWritable(count)); // we use context to write output to file.

  }

{%endhighlight%}

**Tool Runner**
Formalizes the command line arguments into hadoop jobs so you don't need to mess with them yourself they will have the same pattern.
`hadoop jar somejar.jar YourMain -D mapred.reduce.tasks=2 someinputdir someoutputdir`

**State VS Stateless**
map reduce are inherently stateless (pure functions), very nice, you should stick to it as much as possible. however if you need state persisted (but reallymwar, try to find other ways before reverting to state) you will use the `public void setup(Context context)` method its a standard setup method.  Likewise you have a `cleanup` method which is called after map/reduce finishes.  It's quiete common that in `cleanup` if you used some states you will write out the state to disk.

**Scripting languages**

*Hive*

1. Kind of an `SQL` for map reduce jobs.
1. Can add user defined functions.

*Pig*

1. Kind of a `scripting language` for map reduce jobs.

**Other Components**

1. Data transfers - `Flume`, `Scoop`
1. Workflow management - `oozie`, `YARN`

*Impala*

1. Instead of start, read data, write data, those are living servers on hadoop cluster, so much faster (not standard `mapreduce`, not even `mapreduce`)
1. Query language similar to `HiveQL`

